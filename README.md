# ğŸ¤– Interview Co-Pilot

A real-time AI assistant for interviews that provides intelligent responses during live video calls on Google Meet and Zoom.

## ğŸŒŸ Features

### ğŸ¯ **Core Functionality**

- **Real-time Speech Recognition**: Captures interviewer questions using Web Speech API
- **AI-Powered Responses**: Generates intelligent answers using Together.ai (Mixtral 8x7B)
- **Stealth Mode**: Minimal, professional overlay that won't distract or raise suspicion
- **Multi-Platform**: Works on Google Meet, Zoom, and custom test environments

### ğŸ¨ **Advanced UI/UX**

- **Compact Mode**: Default minimal 380x160px overlay showing only AI response
- **Expand Mode**: Full interface with transcription, debug info, and history
- **Confidence Indicators**: Visual feedback on speech recognition quality (ğŸŸ¢ğŸŸ¡ğŸ”´)
- **Token Streaming**: Real-time word-by-word response generation
- **Drag & Dock**: Draggable panel with auto-docking to screen edges
- **Emergency Hide**: Instant hide with Ctrl+Shift+H hotkey

### ğŸ›¡ï¸ **Reliability Features**

- **Fallback Responses**: Smart offline responses when server unavailable
- **Multiple CDN Support**: Socket.IO with primary + backup + unpkg fallbacks
- **Health Monitoring**: Server connectivity checks and error handling
- **Auto-hide**: Fades to 30% opacity during inactivity
- **Context Awareness**: Auto-detects question categories (#API, #Selenium, #Behavioral)

## ğŸš€ Quick Start

### Prerequisites

- Node.js 16+ installed
- Chrome browser with Developer Mode enabled
- Together.ai API key

### 1. Clone and Setup

```bash
git clone https://github.com/shrirajnaik04/interview-copilot.git
cd interview-copilot

# Install server dependencies
npm install

# Install client dependencies
cd client
npm install
```

### 2. Configuration

Create a `.env` file in the root directory:

```bash
TOGETHER_API_KEY=your_together_ai_api_key_here
PORT=3001
```

### 3. Build Extension

```bash
cd client
npm run build
# Or use: build.bat on Windows
```

### 4. Install Chrome Extension

1. Open Chrome and go to `chrome://extensions/`
2. Enable "Developer mode" (top right toggle)
3. Click "Load unpacked"
4. Select the `client/dist` directory

### 5. Start Server

```bash
cd ../
npm start
```

### 6. Test Setup

- Open the test page: `enhanced-test.html`
- Check that extension overlay appears
- Test speech recognition and AI responses

1. Join a Google Meet or Zoom call
2. Click the extension icon to open settings (if needed)
3. The floating overlay will appear automatically
4. Click the ğŸ¤ button to start listening
5. AI responses will appear in real-time as the interviewer speaks

## ğŸ› ï¸ Project Structure

```
interview-copilot/
â”œâ”€â”€ server/                 # Node.js backend
â”‚   â””â”€â”€ index.js           # Express server with Socket.IO
â”œâ”€â”€ client/                # Chrome extension
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ content.js     # Main extension logic
â”‚   â”‚   â”œâ”€â”€ background.js  # Service worker
â”‚   â”‚   â”œâ”€â”€ popup.html     # Settings popup
â”‚   â”‚   â”œâ”€â”€ popup.js       # Settings logic
â”‚   â”‚   â””â”€â”€ styles.css     # UI styles
â”‚   â”œâ”€â”€ manifest.json      # Extension manifest
â”‚   â””â”€â”€ dist/             # Built extension files
â”œâ”€â”€ .env                  # Environment variables
â””â”€â”€ package.json          # Project dependencies
```

## âš™ï¸ Configuration

### Environment Variables (.env)

```bash
TOGETHER_API_KEY=52ba9a34834d11fc356bf0fa8f82383bfdde132a0b5af27706c1c49e32f54fba
PORT=3001
```

### Extension Settings

Access via extension popup:

- **Enable/Disable**: Toggle the assistant
- **Server URL**: Backend server address (default: http://localhost:3001)
- **API Key**: Together.ai API key (optional if set in .env)

## ğŸ® Keyboard Shortcuts

- `Ctrl+Shift+C`: Toggle overlay visibility
- `Ctrl+Shift+L`: Toggle listening/recording

## ğŸ”§ Technical Details

### Speech Recognition

- Uses browser's Web Speech API
- Continuous listening with interim results
- Automatic restart on interruption
- Filters final transcripts for AI processing

### AI Integration

- **Model**: `mistralai/Mixtral-8x7B-Instruct-v0.1`
- **API**: Together.ai (OpenAI-compatible)
- **Prompt Engineering**: Optimized for concise, professional interview responses
- **Rate Limiting**: Built-in to prevent API overuse

### Communication Flow

1. Browser captures audio â†’ Web Speech API
2. Extension transcribes â†’ WebSocket to server
3. Server sends to Together.ai â†’ AI generates response
4. Response streams back â†’ Extension displays in overlay

## ğŸ›¡ï¸ Privacy & Security

- **Local Processing**: Audio stays in browser, only text sent to server
- **No Recording**: No audio files stored or transmitted
- **Minimal Data**: Only transcribed text and responses processed
- **Secure Communication**: HTTPS/WSS in production

## ğŸš€ Production Deployment

### Backend (Server)

```bash
# Set production environment
export NODE_ENV=production
export TOGETHER_API_KEY=your_api_key

# Start with PM2 or similar
npm start
```

### Extension

1. Build for production: `npm run build`
2. Package as .crx file for distribution
3. Submit to Chrome Web Store (optional)

## ğŸ› Troubleshooting

### Common Issues

**Extension not loading:**

- Check Chrome Developer mode is enabled
- Verify manifest.json is valid
- Check browser console for errors

**Speech recognition not working:**

- Ensure microphone permissions granted
- Check if browser supports Web Speech API
- Verify you're on HTTPS site (required for speech API)

**Server connection failed:**

- Verify server is running on correct port
- Check firewall/antivirus blocking connections
- Ensure CORS is properly configured

**AI responses not generating:**

- Verify Together.ai API key is valid
- Check server logs for API errors
- Ensure sufficient API credits

### Debug Mode

1. Open Chrome DevTools (F12)
2. Check Console tab for errors
3. Network tab shows WebSocket connections
4. Storage tab shows extension settings

## ğŸ“ API Reference

### Server Endpoints

**POST /api/generate-answer**

```json
{
  "question": "Tell me about yourself",
  "context": "Software engineering interview"
}
```

**GET /health**

```json
{
  "status": "OK",
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

### WebSocket Events

**Client â†’ Server:**

- `transcription`: Send transcribed text for AI processing

**Server â†’ Client:**

- `answer`: AI-generated response
- `error`: Error messages

## ğŸ”® Future Enhancements

- [ ] **Multi-language support** for international interviews
- [ ] **Custom prompt templates** for different interview types
- [ ] **Interview analytics** and performance tracking
- [ ] **Offline mode** with local AI models
- [ ] **Voice synthesis** for audio responses
- [ ] **Integration** with popular job boards
- [ ] **Team collaboration** features for interview preparation

## ğŸ“„ License

MIT License - feel free to modify and distribute.

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“ Project Structure

```
interview-copilot/
â”œâ”€â”€ server/                 # Node.js backend
â”‚   â””â”€â”€ index.js           # Express server with Socket.IO
â”œâ”€â”€ client/                # Chrome extension
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ content.js     # Main extension logic
â”‚   â”‚   â”œâ”€â”€ styles.css     # Enhanced UI styles
â”‚   â”‚   â”œâ”€â”€ popup.html     # Extension popup
â”‚   â”‚   â””â”€â”€ background.js  # Service worker
â”‚   â”œâ”€â”€ manifest.json      # Extension manifest
â”‚   â”œâ”€â”€ dist/             # Built extension files
â”‚   â””â”€â”€ build.bat         # Build script
â”œâ”€â”€ .env                  # Environment variables (API keys)
â”œâ”€â”€ enhanced-test.html    # Testing environment
â””â”€â”€ README.md            # This file
```

## ğŸ¯ Usage

### During an Interview

1. **Join your video call** (Google Meet/Zoom)
2. **Extension overlay appears** in top-right corner (compact mode)
3. **Click ğŸ¤ button** to start listening
4. **Speak naturally** - AI responds to interviewer questions
5. **Use Ctrl+Shift+H** for emergency hide if needed

### Key Controls

- **ğŸ¤ Toggle**: Start/stop listening
- **â‡² Expand**: Switch between compact/full mode
- **âš™ï¸ Settings**: Configure API key, model, opacity
- **ğŸ‘ï¸ Emergency**: Quick hide button
- **Keyboard Shortcuts**:
  - `Ctrl+Shift+H`: Emergency hide/show
  - `Ctrl+Shift+L`: Toggle listening
  - `Ctrl+Shift+E`: Expand/collapse mode

## ğŸ”§ Configuration

### API Settings

- **API Key**: Enter your Together.ai API key in settings
- **Model Selection**: Choose between Mixtral 8x7B or LLaMA 2 70B
- **Server URL**: Default `http://localhost:3001`

### UI Preferences

- **Opacity**: Adjust transparency (50-100%)
- **Confidence Indicators**: Show/hide speech recognition quality
- **Streaming**: Enable/disable real-time response generation
- **Auto-hide**: Fade overlay when inactive

## ğŸ§ª Testing

### Test Environment

- Open `enhanced-test.html` in your browser
- Use test buttons to verify:
  - Server connection
  - Speech recognition
  - Socket.IO connectivity
  - AI response generation

## âš ï¸ Disclaimer

This tool is for educational and practice purposes. Always follow your organization's policies and interview guidelines. Use responsibly and ethically.

---

**Built with â¤ï¸ for interview success**
